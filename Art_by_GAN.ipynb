{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "ðŸŽ¨Art by GANðŸŽ¨",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION  IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'art-portraits:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1698586%2F7457578%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T125852Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D57cd3c728a27594ee40045cffcdecfd4e6df0cbc0c53f32c557fef05992a19c75cfd0af829105142c729f2f5841cbe71ae0de18f509254b901e4eb8e60937d1281fd065ac2c457506ad9aae170a3bed47060746f28451c537524e9805f0d2dfc87ed1b6f0a61ad07c16e450a54a55ad2e0662a3948d6b8a70b903634022d35aff524b119d4d984c3f9ad7291c1c86c8b9984b199b673097d80171e339ef8bdf7caca49cd4bb231c7e889fb55d0728487634e817b0fcc7678b15136fdc888ab3e63a03992e16e0cb7e49463fdfc797350ba990546bdd3a22e9aa197001db650164c93d05433c76d03ada7952b66a90778bb3d8f7ca944fa4c95dafa2870ecd7df'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "A7DmpRA1PkKN"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk(\"../input/art-portraits/Portraits/\"):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "EfwMcBXMPkKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow  as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout, ZeroPadding2D, Flatten, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#Settings\n",
        "sns.set(rc={\"axes.facecolor\":\"#EDE9DE\",\"figure.facecolor\":\"#D8CA7E\"})"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "asK_dltXPkKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <a id=\"2\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">DATA LOADING & PREPREPROCESSING</p>\n",
        "\n",
        "For this project, I am using .jpg files of images of portraits. The dataset includes various artists. I am loading data as TensorFlow.Dataset,, with a batch size of 64. I have reduced the image size to (64,64), presuming, it will be computationally less taxing on the GPU.\n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Loading the data</p>"
      ],
      "metadata": {
        "id": "7em3moahPkKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing data\n",
        "data_path = \"../input/art-portraits/Portraits/\"\n",
        "batch_s = 64\n",
        "#Import as tf.Dataset\n",
        "data = tf.keras.preprocessing.image_dataset_from_directory(data_path, label_mode = None, image_size = (64,64), batch_size = batch_s)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "SFwpZJUWPkK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I have the dataset loaded, let us have a look at a few images."
      ],
      "metadata": {
        "id": "tkhGOSRMPkK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defing a function to see images\n",
        "def Show_Img(data):\n",
        "    plt.figure(figsize=(15,15))\n",
        "    for images in data.take(1):\n",
        "        for i in range(18):\n",
        "            ax = plt.subplot(6, 6, i + 1)\n",
        "            ax.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            ax.axis(\"off\")\n",
        "#Plotting the images in dataset\n",
        "Show_Img(data)"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "_kg_hide-input": true,
        "trusted": true,
        "id": "3Mupxd6pPkK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the images are portraits. A portrait is a painting representation of a person, The face is predominantly depicted portraits along with expressions and postures. To represent the personality of the subject. Since our model is relative a smaller GAN we have reduced the size of the image.\n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Preprocessing the data</p>\n",
        "\n",
        "**Normalization:** For the data normalization, I will convert the data in the range between 0 to 1. This helps in fast convergence and makes it easy for the computer to do calculations faster.\n",
        "Each of the three RGB channels in the image can take pixel values ranging from 0 to 256. Dividing it by 255 converts it to a range between 0 to 1. By doing this we"
      ],
      "metadata": {
        "id": "MRMR9dAgPkK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the dataset for model\n",
        "data = data.map(lambda x: x / 255.0)\n",
        "data"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "uc_ew-u7PkK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Builing the Generator"
      ],
      "metadata": {
        "id": "LruMVv1sP2gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "g_resolution=2\n",
        "\n",
        "#Building a Generator\n",
        "generator = Sequential()\n",
        "generator.add(Dense(4*4*256,activation=\"relu\",input_dim=latent_dim))\n",
        "generator.add(Reshape((4,4,256)))\n",
        "generator.add(UpSampling2D())\n",
        "generator.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "generator.add(Activation(\"relu\"))\n",
        "generator.add(UpSampling2D())\n",
        "generator.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "generator.add(Activation(\"relu\"))\n",
        "generator.add(UpSampling2D())\n",
        "generator.add(Conv2D(256,kernel_size=3,padding=\"same\"))#\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "generator.add(Activation(\"relu\"))\n",
        "generator.add(UpSampling2D())\n",
        "generator.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "generator.add(BatchNormalization(momentum=0.8))\n",
        "generator.add(Activation(\"relu\"))\n",
        "generator.add(Conv2D(3,kernel_size=3,padding=\"same\"))\n",
        "generator.add(Activation(\"tanh\"))\n",
        "\n",
        "generator.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "9iX58GUUPkLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the Generator is framed, let us see what random output our untrained Generator produces to get an idea of the process."
      ],
      "metadata": {
        "id": "nWjDzsE2PkLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a random seed and output from generator\n",
        "seed = tf.random.normal([1, latent_dim])\n",
        "Generated_Portrait = generator(seed, training=False)\n",
        "#Plotting the image output of generator without training\n",
        "plt.imshow(Generated_Portrait[0, :, :, 0])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "UKZ1YOjLPkLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, the output is a random seed containing noise as the Generator is not trained yet.\n",
        "\n",
        "<a id=\"3.2\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">The Discriminator</p>\n",
        "\n",
        "In GANs the Generator works along with the Discriminator.\n",
        "\n",
        "The Discriminator network decided whether the data is fake aka created by the Generator or real i.e. from the original input data. To do so it applies a binary classification method using a sigmoid function to get an output in the range of 0 to 1.\n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Building a Discriminator</p>"
      ],
      "metadata": {
        "id": "KwSBz4tqPkLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building a Discriminator\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(64,64,3), padding=\"same\"))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.25))\n",
        "discriminator.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "discriminator.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "discriminator.add(BatchNormalization(momentum=0.8))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.25))\n",
        "discriminator.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "discriminator.add(BatchNormalization(momentum=0.8))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.25))\n",
        "discriminator.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "discriminator.add(BatchNormalization(momentum=0.8))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.25))\n",
        "discriminator.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
        "discriminator.add(BatchNormalization(momentum=0.8))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.25))\n",
        "discriminator.add(Flatten())\n",
        "discriminator.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6GW89gFVPkLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with this discriminator(untrained), let us see what verdict it has for the preiously generated image with random noise."
      ],
      "metadata": {
        "id": "1NmsbEjNPkLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for the random image generated\n",
        "Discriminator_Verdict = discriminator(Generated_Portrait)\n",
        "print (Discriminator_Verdict)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ia9nqO-QPkLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the discriminator i.e. The Verdict, Says that there is almost a fifty-fifty chance of the image being real. This is so because the Discriminator is not yet trained. So basically, An untrained Generarator generated some pixel-noise and the untrained Discriminator classified it as \"can't tell\". So far we are on a right track.\n",
        "\n",
        "Let us proceed and build the GAN architecture to train.\n",
        "\n",
        "<a id=\"4\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">GAN COMPILATION</p>\n",
        "\n",
        "GAN training has two sections:\n",
        "\n",
        "**Section 1**: The Discriminator is trained while the Generator is idle.\n",
        "The discriminator is trained real images and random noise (from an untrained generator). This trains it to tell between fake and real. This accommodates the discriminator to predict as fakes.\n",
        "\n",
        "**Section 2**: The Generator is trained while the Discriminator is idle.  In this section, the generator is trained.  After training the Discriminator, this step uses the predictions from the discriminator. Grants the generator to adjust the weights to try to deceive the discriminator.\n",
        "\n",
        "The above method is repeated for a few epochs.  \n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">The next section defines the GAN training</p>"
      ],
      "metadata": {
        "id": "veIskItJPkLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(seed)\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(seed))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}"
      ],
      "metadata": {
        "trusted": true,
        "id": "NeDGtDSzPkLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"5\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">TRAINING THE MODEL</p>\n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Train the model</p> Calling the above created GAN function trains the generator and discriminator simultaneously.\n",
        "\n",
        "To implement the GAN we must define:\n",
        "* Number of epochs\n",
        "* The optimizers for Generator and Discriminator\n",
        "* The cross-entropy loss\n",
        "\n",
        "After defing optimizers and numbers of epochs, We will define, compile and fit the model."
      ],
      "metadata": {
        "id": "SUBNp3v5PkLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the number of epochs\n",
        "epochs = 200\n",
        "#The optimizers for Generator and Discriminator\n",
        "discriminator_opt = tf.keras.optimizers.Adamax(1.5e-4,0.5)\n",
        "generator_opt = tf.keras.optimizers.Adamax(1.5e-4,0.5)\n",
        "#To compute cross entropy loss\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "#Defining GAN Model\n",
        "model = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "\n",
        "#Compiling GAN Model\n",
        "model.compile(d_optimizer=discriminator_opt, g_optimizer=generator_opt, loss_fn=loss_fn)\n",
        "\n",
        "#Fitting the GAN\n",
        "history = model.fit(data, epochs=epochs)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "lziYg_qEPkLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"6\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">EVALUATING THE MODEL</p>\n",
        "\n",
        "Now that I have my model trained, let us see how it performs.\n",
        "Having a look at the performance of the model via Learning Curves\n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">Ploting the Learning Curves</p>"
      ],
      "metadata": {
        "id": "9YzVTbXTPkLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pal=[\"#994F5F\",\"#E2AB30\"]\n",
        "#Plotting the learning curve\n",
        "history_df = pd.DataFrame(history.history)\n",
        "fig = plt.figure(figsize=(15,4))\n",
        "ax=sns.lineplot(data=history_df, palette= pal)\n",
        "ax.set(xlabel =\"Epochs\")\n",
        "ax.set(ylabel =\"Loss\")\n",
        "ax.set_title(\"Learning Curve\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "4AUmY-NoPkLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks alright-ish!\n",
        "\n",
        "Let us get some portraits done by the GAN and appreciate the art created by this AI.\n",
        "To get the art output I will create a function that saves the output portraits generated. We will be plotting the generated Portraits\n",
        "\n",
        "<p style=\"font-family:newtimeroman;color:#95856a;font-size:150%\">AI makes Artwork</p>"
      ],
      "metadata": {
        "id": "VdVG76r7PkLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of images to be generate\n",
        "num_img=18\n",
        "\n",
        "#A function to generate and save images\n",
        "def Potrait_Generator():\n",
        "    Generated_Paintings = []\n",
        "    seed = tf.random.normal([num_img, latent_dim])\n",
        "    generated_image = generator(seed)\n",
        "    generated_image *= 255\n",
        "    generated_image = generated_image.numpy()\n",
        "    for i in range(num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_image[i])\n",
        "            Generated_Paintings.append(img)\n",
        "            img.save(\"Potraits{:02d}.png\".format(i))\n",
        "    return\n",
        "\n",
        "#Generating images\n",
        "Images = Potrait_Generator()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KKAyoJetPkLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading generated images\n",
        "Generated_path = \"./\"\n",
        "Potraits_generated = tf.keras.preprocessing.image_dataset_from_directory(Generated_path, label_mode = None)\n",
        "#Plotting generated images\n",
        "Show_Img(Potraits_generated)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "9K1QvM0OPkLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"7\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">CONCLUSION</p>\n",
        "<p style=\"font-family:newtimeroman;font-size:120%;color:#95856a\">In the evaluation of the model: We can see that the GAN picked up the patterns in the portraits. It worked quite well. For further improvement,  as GANs are notorious for being data-hungry, I would consider increasing the dataset. There are many inconsistencies in the data which is rather complicated for the GAN to learn. Cleaning the data with some consistencies in the portrait styles would certainly help. Training it longer i.e. for more epochs would also help. Lastly, one can always strive to make a  more robust architecture for the Neural Networks. </p>\n",
        "\n",
        "\n",
        "\n",
        "<a id=\"8\"></a>\n",
        "# <p style=\"background-color:#95856a;font-family:newtimeroman;color:#D8CA7E;font-size:120%;text-align:center;border-radius:40px 40px;\">END</p>"
      ],
      "metadata": {
        "id": "7GAs3bGcPkLR"
      }
    }
  ]
}